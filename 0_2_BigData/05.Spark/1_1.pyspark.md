
# pyspark.pandas
## 1.版本要求

Python 3.8 and above.
The default distribution uses Hadoop 3.3 and Hive 2.3
Spark pre-built for Apache Hadoop 3.3 and later (default)

Spark 3.4.0 Python 3.8 Hadoop 3.3 Hive 2.3 Java 8u362
 
```shell
#创建
conda create -n py37 --clone base
conda create -n env-name python=python-version

conda create -n env-name python=python-version

#查看当前存在那些虚拟环境
conda env list

#激活python3.7环境
conda activate py37
#退出python3.7环境
conda deactivate
#删除指定环境
conda remove --name python37 --all
```


```shell
conda install -c conda-forge pyspark
```