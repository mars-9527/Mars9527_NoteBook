
# 一、下载

在这个地址下载Apache下的所有软件
[Index of /dist (apache.org)](http://archive.apache.org/dist/)

下载兼容Scala 2.11的1.13.6版本
https://archive.apache.org/dist/flink/flink-1.13.6/flink-1.13.6-bin-scala_2.11.tgz


# 二、配置

将二进制包解压到目录  `/usr/local/bigdata/flink` 下

环境变量添加FLINK_HOME
```
export FLINK_HOME=/usr/local/bigdata/flink/flink-1.13.6
```

修改配置文件 `$FLINK_HOME/conf/flink-conf.yaml`

```yaml
taskmanager.numberOfTaskSlots: 8
parallelism.default: 2
execution.checkpointing.interval: 10000
state.backend: filesystem
state.checkpoints.dir: hdfs://localhost:9000/tmp/flink-checkpoints-directory
state.savepoints.dir: file://localhost:9000/tmp/flink-savepoints-directory
```


# 三、运行

```shell

cd $FLINK_HOME

bin/start-cluster.sh
bin/stop-cluster.sh

bin/sql-client.sh embedded
bin/sql-client.sh embedded -Denv.flink.sql.client.timeout=60000  # 增大超时时间
```
Flink webUI: http://localhost:8081/


# 四、Flink + Kafka测试


## 1.添加依赖
在Maven中下载如下jar包，放置于$FLINK/lib目录

```
flink-connector-kafka_2.11-1.13.6.jar
flink-sql-connector-kafka_2.11-1.13.6.jar
kafka-clients-3.3.1.jar
flink-shaded-hadoop-3-uber-3.1.1.7.2.9.0-173-9.0.jar
commons-cli-1.5.0.jar
```

在Mac上拷贝完成之后可能不会生效，可以做两个额外的操作尝试一下：
1. 设置$FLINK_HOME/lib下所有jar包的读写权限为 `777`  
2. 重启电脑，然后重新启动所有服务，再次尝试
最有可能的解决办法应该是重启电脑。

## 2.启动服务

```shell
zkServer.sh start
nohup kafka-server-start.sh $KK_HOME/config/server.properties >$KK_HOME/logs/console.log 2>&1 &
cd $FLINK_HOME
bin/start-cluster.sh
```

## 3.Flink SQL测试实例

```sql

# 1.创建kafka connector，连接我们第7步创建的test-topic
# 写到topic中的数据为json string
CREATE TABLE test_kafka (
  `name` STRING,
  `age` INT
) with (
  'connector' = 'kafka',
  'topic' = 'test-topic',
  'properties.bootstrap.servers' = 'localhost:9092',
  'json.ignore-parse-errors' = 'true',
  'format' = 'json',
  'properties.group.id' = 'dev01',
  'scan.startup.mode' = 'earliest-offset',
  'scan.topic-partition-discovery.interval' = '600000'
)
;

# 2.创建print sink，将结果输出到Web UI Task Managers中的Stdout
CREATE TABLE test_print (
  `name` STRING,
  `age` INT
)
WITH(
  'connector' = 'print'
);

# 3.将kafka的数据写入sink
INSERT INTO test_print
SELECT `name`, `age`
FROM test_kafka
;

```

```shell

Flink SQL> CREATE TABLE test_kafka (
>   `name` STRING,
>   `age` INT
> ) with (
>   'connector' = 'kafka',
>   'topic' = 'test-topic',
>   'properties.bootstrap.servers' = 'localhost:9092',
>   'json.ignore-parse-errors' = 'true',
>   'format' = 'json',
>   'properties.group.id' = 'dev01',
>   'scan.startup.mode' = 'earliest-offset',
>   'scan.topic-partition-discovery.interval' = '600000'
> )
> ;
[INFO] Execute statement succeed.

Flink SQL> CREATE TABLE test_print (
>   `name` STRING,
>   `age` INT
> )
> WITH(
>   'connector' = 'print'
> );
[INFO] Execute statement succeed.

Flink SQL> INSERT INTO test_print
> SELECT `name`, `age`
> FROM test_kafka
> ;
[INFO] Submitting SQL update statement to the cluster...
[INFO] SQL update statement has been successfully submitted to the cluster:
Job ID: 73c1a7beb42b176edc4688aa8a35cc95


Flink SQL>
```


可以在看到Web UI 中看到提交的任务
![[Pasted image 20230530162537.png]]


启动Kafka producer，往test-topic中写入一些测试数据

```shell
 ~  kafka-console-producer.sh --broker-list localhost:9092 --topic test-topic
>{"name":"test","age":20}
>{"name":"test2","age":30}
>

```

然后在TaskManager中看到消息
![[Pasted image 20230530162927.png]]


至此，Flink + Kafka联调成功

# 五、Flink + Hudi测试

## 1.添加依赖
Hudi不需要安装，在官网下载对应版本的flink-bundle或者spark-bundle
由于我们的Flink版本是1.13，因此下载hudi-flink1.13-bundle-0.11.1.jar：

[Central Repository: org/apache/hudi/hudi-flink1.13-bundle_2.11 (maven.org)](https://repo1.maven.org/maven2/org/apache/hudi/hudi-flink1.13-bundle_2.11/)

放置于$FLINK/lib目录

## 2.启动服务

重启Flink集群服务
```shell
cd $FLINK
bin/stop-cluster.sh
bin/start-cluster.sh
bin/sql-client.sh embedded -Denv.flink.sql.client.timeout=60000
```

## 3.Flink SQL测试实例

```sql
# 1.创建Hudi COW表
CREATE TEMPORARY TABLE test_flink_incremental (
  `id` BIGINT PRIMARY KEY NOT ENFORCED,
  `name` STRING,
  `price` DOUBLE,
  `ts` BIGINT,
  `dt` STRING
)
PARTITIONED BY (`dt`)
WITH(
  'connector' = 'hudi',
  'path' = 'hdfs://localhost:9000/user/hive/warehouse/hudi.db/test_flink_incremental',
  'table.type' = 'COPY_ON_WRITE',
  'hoodie.datasource.write.recordkey.field' = 'id',
  'write.precombine.field' = 'ts'
)
;

# 2.写入一条数据
INSERT INTO test_flink_incremental VALUES (1,'a1', 10, 1000, '2022-11-25');

# 3.创建Hudi read表
CREATE TABLE read_incremental (
  id BIGINT PRIMARY KEY NOT ENFORCED,
  name STRING,
  price DOUBLE,
  ts BIGINT,
  dt STRING
)
PARTITIONED BY (dt)
WITH (
  'connector' = 'hudi',
  'path' = 'hdfs://localhost:9000/user/hive/warehouse/hudi.db/test_flink_incremental'
);

# 设置table format，方便看数据
set sql-client.execution.result-mode = tableau;

select * from read_incremental;

```


```shell
Flink SQL> CREATE TEMPORARY TABLE test_flink_incremental (
>   `id` BIGINT PRIMARY KEY NOT ENFORCED,
>   `name` STRING,
>   `price` DOUBLE,
>   `ts` BIGINT,
>   `dt` STRING
> )
> PARTITIONED BY (`dt`)
> WITH(
>   'connector' = 'hudi',
>   'path' = 'hdfs://localhost:9000/user/hive/warehouse/hudi.db/test_flink_incremental',
>   'table.type' = 'COPY_ON_WRITE',
>   'hoodie.datasource.write.recordkey.field' = 'id',
>   'write.precombine.field' = 'ts'
> )
> ;
[INFO] Execute statement succeed.

Flink SQL> INSERT INTO test_flink_incremental VALUES (1,'a1', 10, 1000, '2022-11-25');
[INFO] Submitting SQL update statement to the cluster...
[INFO] SQL update statement has been successfully submitted to the cluster:
Job ID: ee8ee375ac86c167c72f18cacaac5925


Flink SQL> CREATE TABLE read_incremental (
>   id BIGINT PRIMARY KEY NOT ENFORCED,
>   name STRING,
>   price DOUBLE,
>   ts BIGINT,
>   dt STRING
> )
> PARTITIONED BY (dt)
> WITH (
>   'connector' = 'hudi',
>   'path' = 'hdfs://localhost:9000/user/hive/warehouse/hudi.db/test_flink_incremental'
> );
[INFO] Execute statement succeed.

Flink SQL> set sql-client.execution.result-mode = tableau;
[INFO] Session property has been set.

Flink SQL> select * from read_incremental;
+----+----------------------+--------------------------------+--------------------------------+----------------------+--------------------------------+
| op |                   id |                           name |                          price |                   ts |                             dt |
+----+----------------------+--------------------------------+--------------------------------+----------------------+--------------------------------+
| +I |                    1 |                             a1 |                           10.0 |                 1000 |                     2022-11-25 |
+----+----------------------+--------------------------------+--------------------------------+----------------------+--------------------------------+
Received a total of 1 row
```


## 4.异常处理

1.ClassNotFoundException:parquet.MapredParquetInputFormat

```
Flink SQL> select * from read_incremental;
[ERROR] Could not execute SQL statement. Reason:
java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat

Flink SQL>
```

【解决】添加hadoop-mapreduce-client-core-xxx.jar和hive-exec-xxx.jar到Flink lib中。

```shell
cp $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.3.2.jar $FLINK_HOME/lib
cp /usr/local/workspace/learn/hudi-0.11.1/hudi-examples/hudi-examples-spark/target/lib/hive-exec-2.3.1-core.jar $FLINK_HOME/lib
```

在Mac上拷贝完成之后可能不会生效，可以做两个额外的操作尝试一下：
1. 设置$FLINK_HOME/lib下所有jar包的读写权限为 `777`  
2. 重启电脑，然后重新启动所有服务，再次尝试
最有可能的解决办法应该是重启电脑。