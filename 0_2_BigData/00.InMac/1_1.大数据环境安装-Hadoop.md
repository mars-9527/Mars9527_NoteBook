---
tags:
  - Hadoop
  - Mac
---
参考

[https://blog.csdn.net/pengjunlee/article/details/105032978?spm=1001.2014.3001.5502](https://blog.csdn.net/pengjunlee/article/details/105032978?spm=1001.2014.3001.5502)

安装版本：hadoop-3.2.1

# 1.环境准备

## 1.1 修改主机名
```shell
 ~  sudo scutil --set HostName localhost
Password:
 ~  hostname
localhost
```

## 1.2 ssh免密登录
```shell
ssh-keygen -t rsa      （一路回车直到完成）
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod og-wx ~/.ssh/authorized_keys
```

## 1.3 JDK环境
```shell
~  java -version
java version "1.8.0_281"
Java(TM) SE Runtime Environment (build 1.8.0_281-b09)
Java HotSpot(TM) 64-Bit Server VM (build 25.281-b09, mixed mode)
```

# 2.安装Hadoop

## 2.1 解压到/usr/local目录下
```
 ~  ll /usr/local/hadoop-3.2.1
total 352
-rw-rw-r--   1 loujianbin  staff   147K 12  5 23:09 LICENSE.txt
-rw-rw-r--   1 loujianbin  staff    21K 12  5 23:09 NOTICE.txt
-rw-rw-r--   1 loujianbin  staff   1.3K 12  5 23:09 README.txt
drwxr-xr-x  13 loujianbin  staff   416B  1  3 18:11 bin
drwxr-xr-x   5 loujianbin  staff   160B  3 25 17:11 data
drwxr-xr-x   4 loujianbin  staff   128B  3 25 15:37 etc
drwxr-xr-x   7 loujianbin  staff   224B  1  3 18:11 include
drwxr-xr-x   3 loujianbin  staff    96B  1  3 18:11 lib
drwxr-xr-x  14 loujianbin  staff   448B  1  3 18:11 libexec
drwxr-xr-x  39 loujianbin  staff   1.2K  3 26 15:54 logs
drwxr-xr-x  29 loujianbin  staff   928B  1  3 17:29 sbin
drwxr-xr-x   5 loujianbin  staff   160B  3 25 18:20 share
drwxr-xr-x   4 loujianbin  staff   128B  3 25 17:11 temp
```

## 2.2 配置环境变量
`open -t ~/.zshrc`

```shell
# HADOOP配置
export HADOOP_HOME=/usr/local/hadoop-3.2.1
export HADOOP_ROOT_LOGGER=INFO,console
export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native/
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native:$HADOOP_COMMON_LIB_NATIVE_DIR"
```

## 2.3 修改相关配置文件

### 2.3.1 hadoop-env.sh
```sh
# The java implementation to use. By default, this environment
# variable is REQUIRED on ALL platforms except OS X!
export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_281.jdk/Contents/Home

# Location of Hadoop.  By default, Hadoop will attempt to determine
# this location based upon its execution path.
export HADOOP_HOME=/usr/local/hadoop-3.2.1
```

### 2.3.2 mapred-site.xml
```xml
<configuration>
        <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
                <final>true</final>
                <description>The runtime framework for executing MapReduce jobs</description>
        </property>
        <property>
                <name>yarn.app.mapreduce.am.env</name>
                <value>HADOOP_MAPRED_HOME=/usr/local/hadoop-3.2.1</value>
        </property>
        <property>
                <name>mapreduce.map.env</name>
                <value>HADOOP_MAPRED_HOME=/usr/local/hadoop-3.2.1</value>
        </property>
        <property>
                <name>mapreduce.reduce.env</name>
                <value>HADOOP_MAPRED_HOME=/usr/local/hadoop-3.2.1</value>
        </property>
</configuration>
```

### 2.3.3 core-site.xml
```xml
<configuration>
    <!-- 指定 namenode 的通信地址 -->
    <property>
        <name>fs.default.name</name>
        <value>hdfs://localhost:9000</value>
    </property>
    <!-- 指定hadoop运行时产生文件的存储路径 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/usr/local/hadoop-3.2.1/temp</value>
    </property>
</configuration>
```

### 2.3.4 hdfs-site.xml
```xml
<configuration>
        <property>
                <name>dfs.permissions.enabled</name>
                <value>false</value>
        </property>
        <property>
                <name>dfs.replication</name>
                <value>1</value>
        </property>        
        <property>
                <name>dfs.namenode.name.dir</name>
                <value>/usr/local/hadoop-3.2.1/data/namenode</value>
        </property>
        <property>
                <name>dfs.datanode.data.dir</name>
                <value>/usr/local/hadoop-3.2.1/data/datanode</value>
        </property>
        <property>
                <name>dfs.namenode.secondary.http-address</name>
                <value>localhost:9001</value>
        </property>
        <property>
                <name>dfs.webhdfs.enabled</name>
                <value>true</value>
        </property>
        <property>
                <name>dfs.http.address</name>
                <value>0.0.0.0:50070</value>
        </property>
</configuration>
```

### 2.3.5 yarn-site.xml
```xml
<configuration>
        <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
        </property>
</configuration>
```

## 2.4 修改Mac快捷指令下的指向

以前安装过homebrew的Hadoop
```shell
rm -rf /usr/local/bin/hadoop
rm -rf /usr/local/bin/hdfs
rm -rf /usr/local/bin/yarn
ln -s /usr/local/hadoop-3.2.1/bin/hadoop /usr/local/bin/hadoop
ln -s /usr/local/hadoop-3.2.1/bin/hdfs /usr/local/bin/hdfs
ln -s /usr/local/hadoop-3.2.1/bin/yarn /usr/local/bin/yarn

ln -s /usr/local/hadoop/hadoop-3.2.1/bin/hadoop /usr/local/bin/hadoop
ln -s /usr/local/hadoop/hadoop-3.2.1/bin/hdfs /usr/local/bin/hdfs
ln -s /usr/local/hadoop/hadoop-3.2.1/bin/yarn /usr/local/bin/yarn
```

## 2.5 启动Hadoop
```shell
# 格式化HDFS
 ~  hdfs namenode -format

# 一次启动hadoop所有进程
 ~  cd /usr/local/hadoop-3.2.1
/usr/local/hadoop-3.2.1  sbin/start-all.sh
 
# 通过 jps 查看启动的进程
/usr/local/hadoop-3.2.1   jps
2803 NodeManager
2501 SecondaryNameNode
2361 DataNode
2700 ResourceManager
6157 Jps
2254 NameNode
```

## 2.6 测试
```shell
/usr/local/hadoop-3.2.1  hadoop fs -ls /
/usr/local/hadoop-3.2.1  hdfs dfs -mkdir /input
/usr/local/hadoop-3.2.1  hdfs dfs -put /usr/local/hadoop-3.2.1/etc/hadoop/hdfs-site.xml /input/
/usr/local/hadoop-3.2.1  hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar grep /input /output 'dfs[a-z.]+'
/usr/local/hadoop-3.2.1  hdfs dfs -cat /output/*
1	dfs.webhdfs.enabled
1	dfs.replication
1	dfs.permissions.enabled
1	dfs.namenode.secondary.http
1	dfs.namenode.name.dir
1	dfs.http.address
1	dfs.datanode.data.dir
```
